{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### referenced by - https://github.com/kuleshov/audio-super-res\n",
    "* 아래 audio-super-res로 명시된것은 위의 참고자료 모델을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. VCTK데이터셋 다운로드 및 준비\n",
    "\n",
    "* 학습에 사용될 VCTK데이터를 다운로드하고 압축을 푸는 과정을 수행한다.  \n",
    "* 만약 다른 데이터셋을 사용하거나 미리 만든 데이터셋을 사용할 경우 이 단계는 수행 할 필요없다.  \n",
    "* 단, 해당 폴더에 VCTK압축 파일을 저장해 두거나 별도의 경로를 Makefile에 지정해야함  \n",
    "<blockquote>\n",
    "<p>./audio-super-res/data/make</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습데이터 파일 리스트 준비 및 .h5파일로 변환\n",
    "### A. audio-super-res의 preprocess단계를 사용할 경우\n",
    "* wav 음성 데이터 파일들의 (경로+파일이름)으로 구성된 텍스트파일을 준비한다  \n",
    "(ex)./audio-super-res/data/vctk/multispeaker/train-files.txt\n",
    "* wav 파일의 원본을 High-resolution, 학습용으로 preprocess된 파일을 Low-resolution 구성\n",
    "* 위의 과정을 위해 기존 프로젝트에서는 single speaker와 multi speaker로 데이터셋을 분리하여 prep_vctk.py로 전처리했다\n",
    "<blockquote>\n",
    "<p>./audio-super-res/data/vctk/speaker1/make</p>\n",
    "<p>./audio-super-res/data/vctk/multispeaker/make</p>\n",
    "</blockquote>\n",
    "* 위의 make작업으로 학습데이터 리스트를 바탕으로 prep_vctk.py에서 원본 데이터를 Low-Resolution으로 변환 후 잘게 자른 다음 h5파일로 구성하여 저장한다 \n",
    "* 해당 방법의 자세한 내용은 상단 reference 페이지를 방문\n",
    "\n",
    "### B. 별도의 전처리된 .wav파일을 사용할 경우\n",
    "* input파일과 output파일을 준비해 ./data/input 과 ./data/output에 저장한다\n",
    "* input파일 목록과 output파일 목록을 준비한다\n",
    "* 아래와 같이 .wav파일들을 .h5파일로 변환한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse\n",
    "import numpy as np\n",
    "import h5py\n",
    "import librosa\n",
    "from scipy import interpolate\n",
    "from scipy.signal import decimate\n",
    "# ----------------------------------------------------------------------------'\n",
    "args = {\n",
    "    'out' : 'prep_audioset.h5',\n",
    "    'out_val':'prep_audioset.val.h5',\n",
    "    'input_file_list' : './data/inputfiles.txt', \n",
    "    'input_val_file_list':'./data/inputfiles.val.txt',\n",
    "    'output_file_list': './data/outputfiles.txt',\n",
    "    'output_val_file_list':'./data/outputfiles.val.txt',\n",
    "    'in_dir': './data',\n",
    "    'interpolate':1,\n",
    "    'dimension':8192,\n",
    "    'stride':3200, # 4096\n",
    "    'scale':2, # 4\n",
    "    'batch_size':1,\n",
    "    'sr':16000,\n",
    "    'sam':1.\n",
    "}\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "def file2List(files):\n",
    "    # add .txt file values in list array\n",
    "    file_list = []\n",
    "    file_extensions = set(['.wav'])\n",
    "    with open(files) as f:\n",
    "        for line in f:\n",
    "            filename = line.strip()\n",
    "            ext = os.path.splitext(filename)[1]\n",
    "            if ext in file_extensions:\n",
    "                file_list.append(os.path.join(args['in_dir'], filename))\n",
    "    num_files = len(file_list)\n",
    "    return file_list, num_files\n",
    "\n",
    "\n",
    "def get_patch_data(file_list, num_files,args,d,s):\n",
    "    patches = list()\n",
    "    for j, file_path in enumerate(file_list):\n",
    "        if j % 10 == 0: print('%d/%d' % (j, num_files))\n",
    "        \n",
    "        # load audio file\n",
    "        x, fs = librosa.load(file_path, sr=args['sr']) # sr = sample rates\n",
    "        \n",
    "        # crop so that it works with scailing ratio\n",
    "        x_len = len(x)\n",
    "        x = x[ : x_len - (x_len % args['scale'])]\n",
    "        \n",
    "        # Do not generate low-res version. we already have them\n",
    "        \n",
    "        # Generate patches\n",
    "        max_i = len(x) - d + 1 # d = dimension\n",
    "        for i in range(0, max_i, s): # s = strides \n",
    "            # keep only a fraction of all the patches\n",
    "            u = np.random.uniform()\n",
    "            if u > args['sam']: continue\n",
    "                        \n",
    "            patch = np.array( x[i : i+d] )\n",
    "            assert len(patch) == d\n",
    "\n",
    "            # print _patch\n",
    "            patches.append(patch.reshape((d,1)))\n",
    "            \n",
    "    return patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_preprocessed(h5_file, inputfiles, outputfiles , args, save_examples=False):\n",
    "    # Make a list of all preprocessed files\n",
    "    input_file_list, num_input_files = file2List(inputfiles)\n",
    "    output_file_list, num_output_files = file2List(outputfiles)\n",
    "\n",
    "    \n",
    "    # patches to extract and their size # lr not used\n",
    "    if args['interpolate']:\n",
    "        d, d_lr = args['dimension'], args['dimension']\n",
    "        s, s_lr = args['stride'], args['stride']\n",
    "    else:\n",
    "        print('not interpolate')\n",
    "        d, d_lr = args['dimension'], (int)(args['dimension'] / args['scale'])\n",
    "        s, s_lr = args['stride'], (int)(args['stride'] / args['scale'])\n",
    "        \n",
    "        \n",
    "    # get patches\n",
    "    input_patches = get_patch_data(input_file_list, num_input_files,args,d_lr,s_lr)\n",
    "    output_patches = get_patch_data(output_file_list, num_output_files,args,d,s)\n",
    "    \n",
    "    # crop # of patches so that it's a multiple of mini-batch size\n",
    "    num_input_patches = len(input_patches)\n",
    "    num_output_patches = len(output_patches)\n",
    "    \n",
    "    print('num_input_patches:', num_input_patches)\n",
    "    print('num_output_patches:', num_output_patches)\n",
    "    print('batch_size:', args['batch_size'])\n",
    "    num_input_to_keep = int(np.floor(num_input_patches / args['batch_size']) * args['batch_size'])\n",
    "    input_patches = np.array(input_patches[:num_input_to_keep])\n",
    "    \n",
    "    num_output_to_keep = int(np.floor(num_output_patches / args['batch_size']) * args['batch_size'])\n",
    "    output_patches = np.array(output_patches[:num_output_to_keep])\n",
    "\n",
    "    if save_examples:\n",
    "        librosa.output.write_wav('example-hr.wav', output_patches[40][0], fs, norm=False)\n",
    "        librosa.output.write_wav('example-lr.wav', input_patches[40][0], fs / args['scale'], norm=False)\n",
    "        print (output_patches[40].shape)\n",
    "        print (input_patches[40].shape)\n",
    "        print (output_patches[40][0][:10])\n",
    "        print (input_patches[40][0][:10])\n",
    "        print ('two examples saved')\n",
    "\n",
    "    print ('input_patches shape:',input_patches.shape)\n",
    "    print ('output_patches shape:',output_patches.shape)\n",
    "\n",
    "    # create the hdf5 file\n",
    "    data_set = h5_file.create_dataset('data', input_patches.shape, np.float32) # lr\n",
    "    label_set = h5_file.create_dataset('label', output_patches.shape, np.float32) # hr\n",
    "\n",
    "    data_set[...] = input_patches\n",
    "    label_set[...] = output_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "0/1\n",
      "num_input_patches: 48\n",
      "num_output_patches: 48\n",
      "batch_size: 1\n",
      "input_patches shape: (48, 8192, 1)\n",
      "output_patches shape: (48, 8192, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create train\n",
    "    with h5py.File(args['out'], 'w') as f:\n",
    "        add_data_preprocessed(f, args['input_file_list'],args['output_file_list'], args, save_examples=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 마찬가지로 평가 데이터셋도 h5파일로 구성한다\n",
    "* 위의 B경우에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "0/1\n",
      "num_input_patches: 48\n",
      "num_output_patches: 48\n",
      "batch_size: 1\n",
      "input_patches shape: (48, 8192, 1)\n",
      "output_patches shape: (48, 8192, 1)\n"
     ]
    }
   ],
   "source": [
    "    # create train\n",
    "    with h5py.File(args['out_val'], 'w') as f:\n",
    "        add_data_preprocessed(f, args['input_val_file_list'],args['output_val_file_list'], args, save_examples=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터셋의 정보를 시각화하여 확인\n",
    "* 입, 출력 데이터가 올바르게 들어갔는지 확인하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
